{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from skimage import io, transform\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from time import gmtime, strftime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Loader Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToTensor(object):\n",
    "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        face_1, face_2 = sample['face_1'], sample['face_2']\n",
    "\n",
    "        # swap color axis because\n",
    "        # numpy image: H x W x C\n",
    "        # torch image: C X H X W\n",
    "        face_1 = transform.resize(face_1, (128, 128), mode='constant')\n",
    "        face_2 = transform.resize(face_2, (128, 128), mode='constant')\n",
    "        \n",
    "        face_1 = face_1.transpose((2, 0, 1))\n",
    "        face_2 = face_2.transpose((2, 0, 1))\n",
    "        label=torch.LongTensor(1, 1).zero_()\n",
    "        label=sample['label']\n",
    "        return {'face_1': torch.from_numpy(face_1),\n",
    "                'face_2': torch.from_numpy(face_2),\n",
    "                 'label':label\n",
    "               }\n",
    "trans = transforms.Compose([ToTensor()])\n",
    "\n",
    "class FacePairsDataset(Dataset):\n",
    "    \"\"\"Face Pairs dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, txt_file, root_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        data=pd.read_csv(txt_file,header=None)\n",
    "        df=data[0].str[:].str.split(' ', expand=True)\n",
    "        df.columns = [\"face_1\", \"face_2\", \"label\"]\n",
    "        self.face_pair_frame = df\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.face_pair_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name_1 = os.path.join(self.root_dir, self.face_pair_frame.loc[idx, 'face_1'])\n",
    "        img_name_2 = os.path.join(self.root_dir, self.face_pair_frame.loc[idx, 'face_2'])\n",
    "        face_1 = io.imread(img_name_1)\n",
    "        face_2 = io.imread(img_name_2)\n",
    "        label=self.face_pair_frame.loc[idx,'label']\n",
    "        label=int(label)\n",
    "        sample = {'face_1': face_1,'face_2':face_2,'label': label}\n",
    "\n",
    "        if self.transform:\n",
    "            \n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loads and transforms the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trans = transforms.Compose([ToTensor()])\n",
    "\n",
    "face_train_dataset = FacePairsDataset(txt_file='train.txt',root_dir='lfw/',transform=trans)\n",
    "face_test_dataset = FacePairsDataset(txt_file='test.txt',root_dir='lfw/',transform=trans)\n",
    "\n",
    "N = 8\n",
    "train_loader = DataLoader(dataset=face_train_dataset, batch_size=N, shuffle=True, num_workers=2)\n",
    "test_loader = DataLoader(dataset=face_test_dataset, batch_size=N, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tester code for batch loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_faces_batch(sample_batched):\n",
    "    \"\"\"Show image with landmarks for a batch of samples.\"\"\"\n",
    "    faces_1_batch, faces_2_batch = sample_batched['face_1'], sample_batched['face_2']\n",
    "    print (faces_1_batch.shape)\n",
    "    print (faces_2_batch.shape)\n",
    "    labels_batch=sample_batched['label']\n",
    "    print(labels_batch.shape)\n",
    "    batch_size = len(faces_1_batch)\n",
    "    im_size = faces_1_batch.size(2)\n",
    "    grid_1 = utils.make_grid(faces_1_batch)\n",
    "    grid_2 = utils.make_grid(faces_2_batch)\n",
    "    plt.imshow(grid_1.numpy().transpose((1, 2, 0)))\n",
    "    plt.imshow(grid_2.numpy().transpose((1, 2, 0)))\n",
    "\n",
    "for i_batch, sample_batched in enumerate(train_loader):\n",
    "    print (i_batch)\n",
    "    # sample_batched['face_1'].size() + sample_batched['face_2'].size()\n",
    "    # observe 4th batch and stop.\n",
    "    if i_batch == 3:\n",
    "        plt.figure()\n",
    "        show_faces_batch(sample_batched)\n",
    "        plt.axis('off')\n",
    "        plt.ioff()\n",
    "        plt.show()\n",
    "        break\n",
    "i=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Model C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # 1 input image channel, 6 output channels, 5x5 square convolution\n",
    "        # kernel\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=5,padding=2,stride=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=5,padding=2,stride=1)\n",
    "        self.conv3 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3,padding=1,stride=1)\n",
    "        self.conv4 = nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3,padding=1,stride=1)\n",
    "        \n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc1 = nn.Linear(131072, 1024)\n",
    "        \n",
    "\n",
    "    def forward_once(self, x):\n",
    "        # Max pooling over a (2, 2) window\n",
    "        b_64 = nn.BatchNorm2d(64)\n",
    "        b_128 = nn.BatchNorm2d(128)\n",
    "        b_256 = nn.BatchNorm2d(256)\n",
    "        b_512 = nn.BatchNorm2d(512)\n",
    "        b_1024 = nn.BatchNorm2d(1024)\n",
    "        \n",
    "        #1,2,3,4\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = b_64(x)\n",
    "        x = F.max_pool2d(x,2)\n",
    "        \n",
    "        #5,6,7,8\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = b_128(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        \n",
    "        #9,10,11,12\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = b_256(x)\n",
    "        x = F.max_pool2d(x,2)\n",
    "        \n",
    "        #13,14,15 (no max pooling)\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x=b_512(x)\n",
    "        \n",
    "        # 16\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        \n",
    "        #17,18,19\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x= b_1024(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def forward(self,input1,input2):\n",
    "        output1 = self.forward_once(input1)\n",
    "        output2 = self.forward_once(input2)\n",
    "\n",
    "        return output1, output2\n",
    "        \n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Siamese_Net Rewrite below function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "class Siamese_Net(torch.nn.Module):\n",
    "    \n",
    "\n",
    "    def __init__(self):\n",
    "        super(Siamese_Net, self).__init__()\n",
    "        if torch.cuda.is_available():\n",
    "            self.net = Net().cuda()\n",
    "        else:\n",
    "            self.net = Net()\n",
    "        self.fc = nn.Linear(2048, 1)\n",
    "\n",
    "\n",
    "    def forward(self, input1, input2):\n",
    "        output_1,output_2=self.net(input1,input2)\n",
    "      \n",
    "        x = torch.cat((output_1,output_2), 1)\n",
    "        m = nn.Sigmoid()\n",
    "        output = m(self.fc(x))\n",
    "        return output\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sanity Checks for parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params = list(snet.parameters())\n",
    "# print(len(params))\n",
    "# print(params[0].size())  # bases weight\n",
    "# print(params[2].size())  # conv2's .weight\n",
    "# print(params[4].size())  # conv3's .weight\n",
    "# print(params[6].size())  # conv4's .weight\n",
    "# print(params[8].size())  # conv5's .weight\n",
    "# print(params[10].size())  # linear 2 .weight\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizer, Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i=1\n",
    "# input_1 = Variable(face_train_dataset[i]['face_1'].unsqueeze(0).float())\n",
    "# input_2 = Variable(face_train_dataset[i]['face_2'].unsqueeze(0).float())\n",
    "# p = snet(input_1,input_2)\n",
    "# print(p)\n",
    "if torch.cuda.is_available():\n",
    "    snet_3 = Siamese_Net().cuda()\n",
    "else:\n",
    "    snet_3= Siamese_Net()\n",
    "\n",
    "a = torch.randn(2,3,128,128)\n",
    "b = torch.randn(2,3,128,128)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    temp=snet_3(Variable(a).cuda(),Variable(b).cuda())\n",
    "else:\n",
    "    temp=snet_3(Variable(a),Variable(b))\n",
    "    \n",
    "print(temp.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "if torch.cuda.is_available():\n",
    "    snet = Siamese_Net().cuda()\n",
    "    criterion = nn.BCELoss().cuda()\n",
    "else:\n",
    "    snet = Siamese_Net()\n",
    "    criterion = nn.BCELoss()\n",
    "    \n",
    "optimizer = optim.SGD(snet.parameters(), lr=0.001, momentum=0.9)\n",
    "print(snet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(2):  # loop over the dataset multiple times\n",
    "    running_loss = 0\n",
    "    for i, sample_batched in enumerate(train_loader):\n",
    "        # get the inputs\n",
    "        faces_1_batch, faces_2_batch = sample_batched['face_1'], sample_batched['face_2']\n",
    "        labels_batch = sample_batched['label']\n",
    "        \n",
    "        # wrap them in Variable\n",
    "        if torch.cuda.is_available():\n",
    "            input1,input2 = Variable(faces_1_batch.float().cuda()),Variable(faces_2_batch.float().cuda())\n",
    "            labels_batch = Variable(labels_batch.float().cuda())\n",
    "        else:\n",
    "            input1,input2 = Variable(faces_1_batch.float()),Variable(faces_2_batch.float())\n",
    "            labels_batch = Variable(labels_batch.float())\n",
    "        \n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = snet(input1,input2)\n",
    "#         print(\"outputs size\")\n",
    "#         print (outputs.size())\n",
    "        outputs_flat=outputs.view(outputs.numel()).float()\n",
    "#         print(outputs_flat)\n",
    "#         print(labels_batch)\n",
    "        loss = criterion(outputs_flat, labels_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        ++i\n",
    "        \n",
    "print('Finished Training')\n",
    "curr_time=strftime(\"%Y_%m_%d_%H_%M_%S\", gmtime())\n",
    "save_file_path='snet_basic_'+curr_time\n",
    "torch.save(snet.state_dict(), save_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "count=0\n",
    "if torch.cuda.is_available():\n",
    "\n",
    "    snet_load = Siamese_Net().cuda()\n",
    "else:\n",
    "    snet_load = Siamese_Net()\n",
    "    \n",
    "snet_load.load_state_dict(torch.load('snet_basic_2017_11_14_19_17_45'))\n",
    "for data in test_loader:\n",
    "    count=count+1\n",
    "    faces_1_batch, faces_2_batch = sample_batched['face_1'], sample_batched['face_2']\n",
    "    labels_batch = sample_batched['label']\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        input1,input2 = Variable(faces_1_batch.float().cuda()),Variable(faces_2_batch.float().cuda())\n",
    "    else:\n",
    "        input1,input2 = Variable(faces_1_batch.float()),Variable(faces_2_batch.float())\n",
    "\n",
    "    outputs = snet_load(input1,input2)  \n",
    "    predicted = torch.sign(torch.sign(outputs.data-0.5))\n",
    "    \n",
    "    total += labels_batch.size(0)\n",
    "    labels_batch= (labels_batch.view(-1,1))\n",
    "    \n",
    "    correct += (predicted.long() == labels_batch.long()).sum()\n",
    "    \n",
    "   \n",
    "ac=100 * (correct / total)\n",
    "print('Accuracy of the network on the 10000 test images: %d %%' % ac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
